---
title: "factcheckr"
author: "Armel Oum Maemble"
output: 
  github_document:
        toc: true
        toc_depth: 4
  
---


```{r, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```



## factcheckr: Sentiment Analysis

This package provides utility functions for analyzing text data including but not limited to functions for tokenizing text, topterms, word frequency and sentiment analysis to name a few.

Sentiment Analysis involves discerning opinions expressed in various texts, categorizing them into different polarities such as positive, negative, or neutral. Also referred to as opinion mining and polarity detection, this process enables the identification of the underlying sentiment within documents, websites, social media feeds, political speeches, reviews and more. Sentiment Analysis is a form of classification, organizing data into distinct classes. These classes may take on a binary form, distinguishing between positive and negative sentiments, or they may encompass multiple categories, such as happy, sad, angry and so forth. 

This package aims to simplify the Sentiment Analysis(SA) effort by providing a utility that can rapidly help text mining such as reviews by saving thousands of hours to readers and therefore allowing consumers to make informed decisions on consumers products.

## Installation

``` {r install, tidy='formatR',eval=FALSE, echo=TRUE}
devtools::install_github("smaemble/factcheckr")
```

## Usage

``` {r attach, echo=T, results='hide', message=F, warning=F, tidy='formatR'}
library(factcheckr)
```

## Getting started.
This package comes with 04 ready to use datasets(Hotel_Reviews, ObamaVictorySpeech, TrumpVictorySpeech) which can serve as a basis to get an easy start. In this example, we will showcase a contrast between Obama victory speech and Trump Victory speech. You can follow the same technique on any product.

### Let's start with basic text manipulation functions

Break a text into words.

```{r}
(strtokwords("This is an example of how to tokenize text in R."))
```

Breaking a text into sentences.

```{r}
text <- "This is a very long character vector. Why is it so long? I think lng. is short for long. I want to split this vector into sentences by using e.g. strssplit. Can someone help me? That would be nice?"
(strtoksentence(text))
```

Removing extra spaces from left
```{r}
(trimTextl(" This   is  String  "))
```


Removing extra spaces from right, middle of the string such as

```{r}
(trimTextr("  This   is  String")) 
```

Remove all extra spaces from any text

```{r}

(trimText(" This   is  String  "))
```


##Sentiment Analysis of President Obama and Trump speeches.

### Data clean up: Package capability
We are going to use the `neatlyStart()` function to clean up our text data. It takes a `corpus` as a parameter which is a text under the analysis and `subject` domain. It then converts the corpus to lower case, then words, remove any extra spaces, remove stopwords as well as non English words and then create a new column called subject and return a tibble. 

``` {r}
nsObamaSpeech <- neatlyStart(corpus=ObamaVictorySpeech, subject="Obama Speech")
nsTrumpSpeech <- neatlyStart(corpus=TrumpVictorySpeech, subject="Trump Speech")
```


### Create a subject annotations for all subjects

 Use the `combineSubjects()` function by passing a list of `neatlyStart()` output using 03 of the lexicon `nrc`, `bing` or `loughran`. This function combines all 03 subjects and returns a subject annotation.
 
```{r}
nrcSubjects <- combineSubjects(list(nsObamaSpeech, nsTrumpSpeech), lex="nrc")
nrcSubjects
```

Using `bing` lexicon gives a comparable results

```{r}
bingSubjects <- factcheckr::combineSubjects(list(nsObamaSpeech, nsTrumpSpeech), lex="bing")
bingSubjects
```

## Data Visualization

Use `ggplot3(text=corpus, graphType = "frequency", top = 50, embs = FALSE, lexicon="nrc", maxWords = 50, cutoffScore=100)` function to view and analyze 07 graph types namely: `"frequency","sentiment", "emotion", "topterms", "movingaverage", "polarity", "wordcloud"`. The `lexicon` parameter can be `nrc`, `bing` and `loughran`. Words are classified based on the lexicon which contained known words. It is recommended to start with a lower cut off score or you may see an empty plot. Future releases will include suggestive `cutoffScores`.

### Viewing Sentiment Score
To view the sentiment plot, pass the neatlyStart() output to the text argument of the ggplot3() function

```{r}
ggplot3(text=nsObamaSpeech, graphType = "sentiment", lexicon="bing", cutoffScore = 1)
```

```{r, echo=FALSE}
ggplot3(text=nsTrumpSpeech, graphType = "sentiment", lexicon="bing", cutoffScore = 1)
```

### Word Frequency

Graph below shows the most frequently used words in Trump victory speech. 

``` {r}
ggplot3(text=nsTrumpSpeech, graphType = "frequency", top = 35)
```
To view the most frequently used words in Obama victory speech pass the result of the neatlyStart() function to the text argument.
`ggplot3(text=neatlyStart(corpus=ObamaVictorySpeech, subject="Obama Speech"), graphType = "frequency", top = 30)`

``` {r}
ggplot3(text=nsObamaSpeech, graphType = "frequency", top = 30)
```

In this top 30 words of Obama speech, we have `americans` and `american` and it is clear it frequence is equality likely. We can remove American in the analysis and consider it a stopword. use `removeStopwords()` function to remove one or many stopwords as follows. If you run the code below you should see that the words american is no longer present in the histogram. 

```{r}
nsObamaSpeech <- removeStopwords(words_dictionary= nsObamaSpeech, stopwords=data.frame(word = c("american")))
ggplot3(text=nsObamaSpeech, graphType = "frequency", top = 30)
```

### Emotion visualizations accross subjects

To summarize the results of the Sentiment Analysis, the percentages of the prevalence of emotions across these speeches is calculated using `nrc` lexicon because it has multiclass classification with words such as `anger`, `anticipation`, `disgust`, `fear`, `joy`, `sadness`, `surprise` and `trust` while `bing` lexicon classifies only as `positive` or `negative`.

Use `emotionFrequency()` passing `nrcSubjects` obtained above. Then pass that result to the `ggplot3()` to visualize the contrast amongst the subjects under fact checking. 

```{r}
nrcEmotions <- emotionFrequency(subjectsAnnotations=nrcSubjects)
ggplot3(text=nrcEmotions, graphType ="emotion")
```



### Visualizing Emotion by subject

You must pass the `embs=TRUE` parameter to view by subject the plot.

```{r}
ggplot3(text=emotionFrequency(subjectsAnnotations=nrcSubjects), embs=TRUE, graphType ="emotion")
```



We can also examine the words that have influenced the emotionality scores. In simpler terms, we explore which words carry the most significance for the emotion scores within each speech. To enhance interpretability, we will exclude specific core emotion categories as well as polarity.


```{r}
ggplot3(text = topterms(nrcSubjects), graphType ="topterms")
```



## Moving Average Plot
The moving average plot tells us how emotions changed in the speech overtime.

```{r}
ggplot3(text=polarityChange(nrcSubjects), graphType="movingaverage")
```



## WordCloud Visualization. 
For the ultimate visualization, we'll generate a word cloud that highlights the most frequently occurring positive and negative words. Specifically, we'll utilize the `ggplot3()` function to craft a single word cloud encompassing both negative and positive words, presenting a comprehensive view.

```{r warning=F}
ggplot3(text=nsObamaSpeech, graphType = "wordcloud", lexicon="bing", maxWords = 100)
```
Victory is truly highlighted in Obama Victory Speech. 


```{r warning=F}
ggplot3(text=nsTrumpSpeech, graphType = "wordcloud", lexicon="bing", maxWords = 100)
```
If you had run the script, Trump victory speech has more negative tone. Let's remove the word unbelievable from his speech.

```{r}
nsTrumpSpeech <- removeStopwords(words_dictionary= nsTrumpSpeech, stopwords = data.frame(word = c("unbelievable")))
ggplot3(text=nsTrumpSpeech, graphType = "wordcloud", lexicon="bing", maxWords = 100)
```

Even removing unbelievable did not change Trump Speech.


### More factcheckr capabilities

```{r}
# nsObama <- neatlyStart(corpus=ObamaVictorySpeech, subject="Obama Speech")
# nsTrump <- neatlyStart(corpus=TrumpVictorySpeech, subject="Trump Speech")
# nrcSub <- combineSubjects(list(nsObamaSpeech, nsTrumpSpeech), lex="nrc")
#    
# 
# polarityChange(nrcSub)
# polarityPlot(subjects=emotionFrequency(subjectsAnnotations=nrcSubjects))
# output <- neatlyStart(corpus=ObamaVictorySpeech, subject="Obama Speech")
 
# toptermplot(topwords=topterms(combineSubjects(list(output), lex="nrc")))
```

## Questions and Support

Contact the Maintainer via LinkedIn https://www.linkedin.com/in/armel-oum-maemble/
